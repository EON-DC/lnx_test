{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Layer, BatchNormalization, Activation, InputLayer\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers import RandomContrast, RandomFlip, RandomRotation, RandomZoom\n",
    "from keras.layers import Concatenate, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers import Resizing, Rescaling\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.optimizers import Adam, AdamW, SGD\n",
    "from keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import L2, L1\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(\"dataset/spaceship-titanic/train.csv\")  # (8693, 14)\n",
    "df_test_raw = pd.read_csv(\"dataset/spaceship-titanic/test.csv\")  # (4277, 13)\n",
    "# preprocess\n",
    "df_test_raw[\"Transported\"] = False  # (4277, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.concat([df_train_raw, df_test_raw])  # (12970, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_col_features(df_total, selected_col):\n",
    "#     print(f\"\\nAbout {selected_col}\")\n",
    "#     print(f\"is unique: {df_total[selected_col].is_unique}\")\n",
    "#     if df_total[selected_col].is_unique is False:\n",
    "#         print(f\"values: {df_total[selected_col].value_counts()}\")\n",
    "#         print(f\"desc: {df_total[selected_col].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"PassengerId\",  # unique,\n",
    "    \"HomePlanet\",  # Earth     6865 Europa    3133 Mars      2684\n",
    "    \"CryoSleep\",  # False    8079 True     4581\n",
    "    \"Cabin\",  # cnt: 9825\n",
    "    \"Destination\",  # TRAPPIST-1e      8871  55 Cancri e      2641 PSO J318.5-22    1184\n",
    "    \"Age\",  # cnt: 80\n",
    "    \"VIP\",  # False    12401 True       273\n",
    "    \"RoomService\",\n",
    "    \"FoodCourt\",\n",
    "    \"ShoppingMall\",\n",
    "    \"Spa\",\n",
    "    \"VRDeck\",\n",
    "    \"Name\",\n",
    "    \"Transported\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cabin distributing\n",
    "cabin_values = df_total[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "df_total[\"Cabin_deck\"] = cabin_values[0]  # A B C D E F G T\n",
    "df_total = df_total.drop(columns=[\"Cabin\"])  # drop Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abcd 대신 0,1,2,3,4,5,6,7로 매핑\n",
    "deck_mapping = {\n",
    "    deck: idx\n",
    "    for idx, deck in enumerate(sorted(df_total[\"Cabin_deck\"].dropna().unique()))\n",
    "}\n",
    "df_total[\"Cabin_deck\"] = df_total[\"Cabin_deck\"].map(deck_mapping)  # map to int\n",
    "df_total[\"Cabin_deck\"] = df_total[\"Cabin_deck\"].fillna(-1)  # fillna with -1\n",
    "df_total[\"Cabin_deck\"] = df_total[\"Cabin_deck\"].astype(int)\n",
    "\n",
    "df_total.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.columns.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from md_util import write_summary_md\n",
    "\n",
    "# write_summary_md(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log 스케일이 필요한 칼럼들 끼리, 실제 상관이 있는지 궁금\n",
    "feature_log_scale = [\n",
    "    \"RoomService\",\n",
    "    \"FoodCourt\",\n",
    "    \"ShoppingMall\",\n",
    "    \"Spa\",\n",
    "    \"VRDeck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_log1p_corr(df, cols):\n",
    "    cols = feature_log_scale\n",
    "    X = df[cols].copy()\n",
    "\n",
    "    # 결측 처리(상관 계산을 위해 임시로)\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    X_log = np.log1p(X)\n",
    "\n",
    "    corr = X_log.corr(method=\"pearson\")  # 또는 spearman\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", square=True)\n",
    "    plt.title(\"Correlation (log1p)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_log1p_corr(df_total, feature_log_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = feature_log_scale\n",
    "display(df_total[cols].isna().sum())\n",
    "(df_total[cols].isna().sum(axis=1) == len(cols)).mean()  # 5개 전부 NaN 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "cols = feature_log_scale\n",
    "\n",
    "X = df_total[cols].copy()\n",
    "\n",
    "# 1) log1p (NaN은 그대로 유지)\n",
    "X_log = np.log1p(X)\n",
    "\n",
    "# 2) MICE imputation (log space)\n",
    "imp = IterativeImputer(\n",
    "    estimator=BayesianRidge(),\n",
    "    max_iter=20,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_log_imp = pd.DataFrame(imp.fit_transform(X_log), columns=cols, index=X.index)\n",
    "\n",
    "# 3) inverse transform + 음수 방지\n",
    "X_imp = np.expm1(X_log_imp).clip(lower=0)\n",
    "\n",
    "# 4) 원본에 반영 (결측인 곳만 덮어쓰기)\n",
    "df_total[cols] = df_total[cols].where(~df_total[cols].isna(), X_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_train_raw.shape)\n",
    "# display(df_train_raw.head())\n",
    "# display(df_test_raw.shape)\n",
    "# display(df_test_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"HomePlanet\", \"CryoSleep\", \"VIP\", \"Destination\"]\n",
    "drop_cols = [\"PassengerId\", \"Name\"]\n",
    "df_total_drops = df_total.drop(columns=drop_cols)\n",
    "df_total_filled = df_total_drops.fillna(0)\n",
    "# one hot encoding\n",
    "df_onehot_encoded = pd.get_dummies(\n",
    "    df_total_filled, columns=categorical_cols, drop_first=False\n",
    ")\n",
    "display(\"after : one hot encoding\")\n",
    "display(df_onehot_encoded.head())\n",
    "display(\"NaN count total:\", df_onehot_encoded.isna().value_counts())\n",
    "\n",
    "target_col = \"Transported\"  # 실제 라벨 컬럼명\n",
    "\n",
    "y = df_onehot_encoded[target_col].astype(\"float32\").to_numpy()\n",
    "X = df_onehot_encoded.drop(columns=[target_col]).astype(\"float32\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATION = {\n",
    "    \"TRAIN_CNT\": 8693,\n",
    "    \"TEST_CNT\": 4277,\n",
    "    \"TRAIN_RATIO\": 0.8,\n",
    "    \"VAL_RATIO\": 0.2,\n",
    "    \"RANDOM_SEED\": 44,\n",
    "    \"SHUFFLE\": True,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"EPOCHS\": 100,\n",
    "    \"INITIAL_LEARNING_RATE\": 1e-3,\n",
    "    \"PATIENCE\": 10,\n",
    "}\n",
    "DATASET_SIZE = (len(df_onehot_encoded),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, test\n",
    "tensor_data = tf.constant(df_onehot_encoded, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(tensor_data):\n",
    "    X = tensor_data[:, 0:-1]\n",
    "    y = tensor_data[:, -1]\n",
    "\n",
    "    display(f\"Shape of X: {X.shape}\")\n",
    "    display(f\"Shape of y: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_datasets(X, y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if CONFIGURATION[\"SHUFFLE\"]:\n",
    "        ds = ds.shuffle(buffer_size=len(y), seed=CONFIGURATION[\"RANDOM_SEED\"])\n",
    "\n",
    "    return ds.batch(CONFIGURATION[\"BATCH_SIZE\"]).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def split_train_val_test(X, y):\n",
    "    total = int(X.shape[0])\n",
    "    val_n = int(CONFIGURATION[\"TRAIN_CNT\"] * CONFIGURATION[\"VAL_RATIO\"])\n",
    "    test_n = CONFIGURATION[\"TEST_CNT\"]\n",
    "    train_n = CONFIGURATION[\"TRAIN_CNT\"] - val_n\n",
    "\n",
    "    X_train, y_train = X[:train_n], y[:train_n]\n",
    "    X_rest, y_rest = X[train_n:], y[train_n:]\n",
    "    X_val, y_val = X_rest[:val_n], y_rest[:val_n]\n",
    "    X_test, y_test = X_rest[val_n:], y_rest[val_n:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_xy(tensor_data)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_train_val_test(X, y)\n",
    "train_ds = make_datasets(X_train, y_train)\n",
    "val_ds = make_datasets(X_val, y_val)\n",
    "test_ds = make_datasets(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_raw_from_log(y_true_log, y_pred_log):\n",
    "    y_true_raw = tf.math.expm1(y_true_log)\n",
    "    y_pred_raw = tf.math.expm1(y_pred_log)\n",
    "    y_pred_raw = tf.maximum(y_pred_raw, 0.0)\n",
    "    return tf.reduce_mean(tf.abs(y_true_raw - y_pred_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Making Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = int(X.shape[1])\n",
    "\n",
    "inputs = keras.Input(shape=(D,), dtype=tf.float32)\n",
    "x = layers.Dense(128, activation=\"relu\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(1e-3),  # type: ignore\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.AUC(name=\"auc\"),\n",
    "    ],\n",
    ")\n",
    "callbacks = (\n",
    "    [\n",
    "        keras.callbacks.TerminateOnNaN(),\n",
    "        # Early stopping\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=CONFIGURATION[\"PATIENCE\"],\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        # Model checkpoint\n",
    "        ModelCheckpoint(\n",
    "            filepath=\"best_model.h5\",\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "# 3) 학습\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=50, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "predict_result = model.predict(X_test)\n",
    "predict_result = tf.sigmoid(predict_result).numpy().flatten()\n",
    "predict_result_binary = predict_result >= 0.5\n",
    "\n",
    "submission_df = df_test_raw[[\"PassengerId\"]].copy()\n",
    "submission_df[\"Transported\"] = predict_result_binary\n",
    "submission_df.to_csv(\"spaceship_titanic_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
