{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Layer, BatchNormalization, Activation, InputLayer\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.layers import RandomContrast, RandomFlip, RandomRotation, RandomZoom\n",
    "from keras.layers import Concatenate, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers import Resizing, Rescaling\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.optimizers import Adam, AdamW, SGD\n",
    "from keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import L2, L1\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"dataset/Emotions Dataset/Emotions Dataset/train/\"\n",
    "test_dir = \"dataset/Emotions Dataset/Emotions Dataset/test/\"\n",
    "# check exist\n",
    "if os.path.exists(train_dir):\n",
    "    print(\"Training directory found.\")\n",
    "else:\n",
    "    print(\"Training directory not found.\")\n",
    "    raise FileNotFoundError(\"Training directory not found.\")\n",
    "if os.path.exists(test_dir):\n",
    "    print(\"Testing directory found.\")\n",
    "else:\n",
    "    print(\"Testing directory not found.\")\n",
    "    raise FileNotFoundError(\"Testing directory not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check images size\n",
    "# df_image_sizes = []\n",
    "# for class_name in os.listdir(train_dir):\n",
    "#     class_path = os.path.join(train_dir, class_name)\n",
    "#     if os.path.isdir(class_path):\n",
    "#         image_count = len(os.listdir(class_path))\n",
    "#         for i in range(image_count):\n",
    "#             sample_image_path = os.path.join(class_path, os.listdir(class_path)[i])\n",
    "#             sample_image = keras.preprocessing.image.load_img(sample_image_path)\n",
    "#             df_image_sizes.append(\n",
    "#                 {\n",
    "#                     \"class\": class_name,\n",
    "#                     \"width\": sample_image.size[0],\n",
    "#                     \"height\": sample_image.size[1],\n",
    "#                 }\n",
    "#             )\n",
    "# df_image_sizes = pd.DataFrame(df_image_sizes)\n",
    "# df_image_sizes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\"angry\", \"happy\", \"sad\"]\n",
    "CONFIGURATION = {\n",
    "    \"IMAGE_SIZE\": 224,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"N_FILTERS\": 16,\n",
    "    \"KERNEL_SIZE\": 3,\n",
    "    \"N_STRIDES\": 4,\n",
    "    \"REGULATION_RATE\": 1e-3,\n",
    "    \"DROPOUT_RATE\": 0.4,\n",
    "    \"POOL_SIZE\": 2,\n",
    "    \"N_CLASSES\": len(CLASS_NAMES),\n",
    "    \"N_DENSE_1\": 2048,\n",
    "    \"N_DENSE_2\": 128,\n",
    "    \"EPOCHS\": 100,\n",
    "    \"INITIAL_LEARNING_RATE\": 1e-3,\n",
    "    \"DECAY_STEPS\": 500,\n",
    "    \"DECAY_RATE\": 0.95,\n",
    "    \"STAIRCASE\": True,\n",
    "    \"VALIDATION_SPLIT\": 0.01,\n",
    "    \"PATIENCE\": 10,\n",
    "    \"SEED\": 99,\n",
    "    \"DO_COMPILE\": False,\n",
    "    \"USING_MODEL_PATH\": \"models/best_lenet_model_acc87.keras\",\n",
    "    \"DO_DRAW_FEATURE_MAP\": False,\n",
    "    \"DO_DRAW_GRAD_CAM\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=CLASS_NAMES,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=CONFIGURATION[\"SEED\"],\n",
    "    validation_split=CONFIGURATION[\"VALIDATION_SPLIT\"],\n",
    "    subset=\"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=CLASS_NAMES,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=CONFIGURATION[\"SEED\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클래스의 샘플 수 확인\n",
    "for class_name in CLASS_NAMES:\n",
    "    class_path = os.path.join(train_dir, class_name)\n",
    "    count = len(os.listdir(class_path))\n",
    "    print(f\"{class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 불균형 해결\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    \"balanced\",\n",
    "    classes=np.array([0, 1, 2]),\n",
    "    y=np.array([0] * 1525 + [1] * 3019 + [2] * 2255),\n",
    ")\n",
    "class_weight_dict = {i: class_weights[i] for i in range(3)}\n",
    "\n",
    "# 학습 시 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_dataset.take(1):  # type: ignore\n",
    "#     images, labels = i\n",
    "#     print(images.shape)\n",
    "#     print(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dataset))\n",
    "print(\"Image batch shape: \", x.shape)\n",
    "print(\"Label batch shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = (\n",
    "    np.unique(y.numpy())\n",
    "    if len(y.shape) == 1\n",
    "    else np.unique(np.argmax(y.numpy(), axis=1))\n",
    ")\n",
    "print(\"unique labels:\", unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_history(history):\n",
    "    # plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"acc\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history.history[\"val_acc\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):  # type: ignore\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(f\"{CLASS_NAMES[tf.argmax(labels[i])]}(L:{labels[i].numpy()})\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  # type: ignore\n",
    "validation_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale_layers = keras.Sequential(\n",
    "    [\n",
    "        Resizing(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "        Rescaling(1.0 / 255),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Augmentation 추가 (Dataset Preparation 직후)\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.25),\n",
    "        layers.RandomZoom(0.03),\n",
    "        layers.RandomContrast(0.1),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=CONFIGURATION[\"INITIAL_LEARNING_RATE\"],\n",
    "    decay_steps=CONFIGURATION[\"DECAY_STEPS\"],\n",
    "    decay_rate=CONFIGURATION[\"DECAY_RATE\"],\n",
    "    staircase=CONFIGURATION[\"STAIRCASE\"],\n",
    ")\n",
    "\n",
    "\n",
    "# optimizer = Adam(learning_rate=lr_schedule)\n",
    "# optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "loss = CategoricalCrossentropy(from_logits=False, label_smoothing=0.0)\n",
    "metrics = [\n",
    "    CategoricalAccuracy(name=\"acc\"),\n",
    "    CategoricalCrossentropy(name=\"cce\"),\n",
    "]\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=CONFIGURATION[\"PATIENCE\"],\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"best_lenet_model.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "callbacks = [\n",
    "    early_stopping,\n",
    "    model_checkpoint,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_build_model():\n",
    "    if CONFIGURATION[\"DO_COMPILE\"]:\n",
    "        print(\"Model path not found. Building a new model.\")\n",
    "        loaded_model = _setup_lenet_model()\n",
    "    else:\n",
    "        if os.path.exists(CONFIGURATION[\"USING_MODEL_PATH\"]):\n",
    "            print(\"Loading model from:\", CONFIGURATION[\"USING_MODEL_PATH\"])\n",
    "            loaded_model = keras.models.load_model(CONFIGURATION[\"USING_MODEL_PATH\"])\n",
    "            return loaded_model\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Model path not found.\")\n",
    "\n",
    "    # Build a new model if not loading\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def _setup_lenet_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            InputLayer((CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"], 3)),\n",
    "            resize_rescale_layers,\n",
    "            data_augmentation,\n",
    "            Conv2D(\n",
    "                filters=CONFIGURATION[\"N_FILTERS\"],\n",
    "                kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=L2(CONFIGURATION[\"REGULATION_RATE\"]),\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(CONFIGURATION[\"POOL_SIZE\"], CONFIGURATION[\"N_STRIDES\"]),\n",
    "            Conv2D(\n",
    "                filters=CONFIGURATION[\"N_FILTERS\"] * 2,\n",
    "                kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=L2(CONFIGURATION[\"REGULATION_RATE\"]),\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(CONFIGURATION[\"POOL_SIZE\"], CONFIGURATION[\"N_STRIDES\"]),\n",
    "            Conv2D(\n",
    "                filters=CONFIGURATION[\"N_FILTERS\"] * 8,\n",
    "                kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=L2(CONFIGURATION[\"REGULATION_RATE\"]),\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(CONFIGURATION[\"DROPOUT_RATE\"]),\n",
    "            Conv2D(\n",
    "                filters=CONFIGURATION[\"N_FILTERS\"] * 8,\n",
    "                kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=L2(CONFIGURATION[\"REGULATION_RATE\"]),\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(CONFIGURATION[\"DROPOUT_RATE\"]),\n",
    "            Conv2D(\n",
    "                filters=CONFIGURATION[\"N_FILTERS\"] * 8,\n",
    "                kernel_size=CONFIGURATION[\"KERNEL_SIZE\"],\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=L2(CONFIGURATION[\"REGULATION_RATE\"]),\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Flatten(),\n",
    "            Dense(\n",
    "                CONFIGURATION[\"N_DENSE_1\"],\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=L2(CONFIGURATION[\"REGULATION_RATE\"]),\n",
    "            ),\n",
    "            Dropout(0.2),\n",
    "            Dense(\n",
    "                CONFIGURATION[\"N_DENSE_2\"],\n",
    "                activation=\"relu\",\n",
    "            ),\n",
    "            Dense(CONFIGURATION[\"N_CLASSES\"], activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,  # type: ignore\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "lenet_model = load_or_build_model()\n",
    "lenet_model.summary()\n",
    "print(lenet_model.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # low loss test\n",
    "# y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "# y_pred = np.array([[0.9, 0.05, 0.05], [0.1, 0.8, 0.1], [0.2, 0.2, 0.6]])\n",
    "# loss_value = loss(y_true, y_pred).numpy()\n",
    "# print(f\"Low loss value: {loss_value}\")\n",
    "\n",
    "# # high loss test\n",
    "# y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "# y_pred = np.array([[0.1, 0.8, 0.1], [0.3, 0.01, 0.69], [0.9, 0.1, 0.0]])\n",
    "# loss_value = loss(y_true, y_pred).numpy()\n",
    "# print(f\"High loss value: {loss_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 pass mode면 해당 셀에서 lenet_model 불러오기 빌드 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. 배치의 실제 이미지 범위 확인\n",
    "# # 2. 시각화하여 이미지가 올바른지 확인\n",
    "# plt.figure(figsize=(12, 3))\n",
    "# for images, labels in training_dataset.take(1):\n",
    "#     print(\"Image dtype:\", images.dtype)\n",
    "#     print(\"Image min:\", images.numpy().min())\n",
    "#     print(\"Image max:\", images.numpy().max())\n",
    "#     print(\"Label shape:\", labels.shape)\n",
    "#     print(\"Label sample:\", labels.numpy()[:3])\n",
    "#     print(\"Predicted class from label:\", np.argmax(labels.numpy()[:3], axis=1))\n",
    "#     for i in range(3):\n",
    "#         plt.subplot(1, 3, i + 1)\n",
    "#         img = images[i].numpy()\n",
    "#         if img.max() > 1:  # 0-255 범위\n",
    "#             img = img / 255.0\n",
    "#         plt.imshow(img.astype(\"float32\"))\n",
    "#         plt.title(CLASS_NAMES[np.argmax(labels[i].numpy())])\n",
    "#     break\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_ds = training_dataset.take(2).repeat()  # 배치 2개를 반복해서 외우게 함\n",
    "# history = lenet_model.fit(\n",
    "#     small_ds, steps_per_epoch=20, epochs=50, verbose=1, class_weight=class_weight_dict\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 보고 load면 fit 패스\n",
    "if not CONFIGURATION[\"DO_COMPILE\"]:\n",
    "    print(\"Model loaded. Skipping training as per configuration.\")\n",
    "else:\n",
    "    print(\"Starting training...\")\n",
    "    history = lenet_model.fit(\n",
    "        training_dataset,\n",
    "        epochs=CONFIGURATION[\"EPOCHS\"],\n",
    "        validation_data=validation_dataset,\n",
    "        callbacks=callbacks,\n",
    "        # class_weight=class_weight_dict,\n",
    "        verbose=1,\n",
    "    )\n",
    "    visualize_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show false predictions\n",
    "plt.figure(figsize=(10, 10))\n",
    "i = 0\n",
    "for images, labels in validation_dataset:  # type: ignore\n",
    "    f_maps = lenet_model.predict(images)\n",
    "    for j in range(len(images)):\n",
    "        if tf.argmax(f_maps[j]) != tf.argmax(labels[j]):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[j].numpy().astype(\"uint8\"))\n",
    "            plt.title(\n",
    "                f\"Predicted: {CLASS_NAMES[tf.argmax(f_maps[j])]}, Actual: {CLASS_NAMES[tf.argmax(labels[j])]}\"\n",
    "            )\n",
    "            plt.axis(\"off\")\n",
    "            i += 1\n",
    "            if i >= 9:\n",
    "                break\n",
    "    if i >= 9:\n",
    "        break\n",
    "plt.savefig(\"false_predictions_acc87.png\")\n",
    "plt.show()\n",
    "# plt save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt save\n",
    "plt.savefig(\"false_predictions_acc87.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "labels = []\n",
    "for images, label in validation_dataset:  # type: ignore\n",
    "    preds = lenet_model.predict(images, verbose=0)\n",
    "    predicted.extend(np.argmax(preds, axis=1).tolist())\n",
    "    labels.extend(np.argmax(label.numpy(), axis=1).tolist())\n",
    "\n",
    "cm = confusion_matrix(labels, predicted)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    xticklabels=CLASS_NAMES,\n",
    "    yticklabels=CLASS_NAMES,\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_output = classification_report(\n",
    "    labels, predicted, target_names=CLASS_NAMES\n",
    ")\n",
    "print(\"Classification Report:\\n\", classification_report_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Visualization about model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIGURATION[\"DO_DRAW_FEATURE_MAP\"]:\n",
    "    for i in lenet_model.layers:\n",
    "        print(i.name)\n",
    "    display(lenet_model.input_shape)\n",
    "    # dummy 호출\n",
    "    dummy = tf.zeros((1, 224, 224, 3))\n",
    "    _ = lenet_model.predict(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIGURATION[\"DO_DRAW_FEATURE_MAP\"]:\n",
    "    # feature model visualization\n",
    "    feature_layers = [\n",
    "        layer.output for layer in lenet_model.layers if \"conv\" in layer.name\n",
    "    ]\n",
    "    feature_model = Model(\n",
    "        inputs=lenet_model.layers[0].input,\n",
    "        outputs=feature_layers,\n",
    "    )\n",
    "    feature_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIGURATION[\"DO_DRAW_FEATURE_MAP\"]:\n",
    "    test_img = train_dir + r\"/happy/3159.jpg\"\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        test_img, target_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"])\n",
    "    )\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "    f_maps = feature_model.predict(img_array)\n",
    "\n",
    "    # origin\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img_array[0].numpy().astype(\"uint8\"))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIGURATION[\"DO_DRAW_FEATURE_MAP\"]:\n",
    "    for i in range(len(f_maps)):\n",
    "        plt.figure(figsize=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]))\n",
    "        f_size = f_maps[i].shape[-1]\n",
    "        n_channels = int(np.sqrt(f_size))\n",
    "        for j in range(n_channels * n_channels):\n",
    "            plt.subplot(n_channels, n_channels, j + 1)\n",
    "            plt.imshow(f_maps[i][0, :, :, j], cmap=\"gray\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.suptitle(f\"Feature Maps from Conv Layer {i+1}\")\n",
    "        plt.savefig(f\"feature_map_conv_layer_{i+1}.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "DRAWING Grad CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lenet_model.layers:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIGURATION[\"DO_DRAW_GRAD_CAM\"]:\n",
    "    from keras import Input\n",
    "\n",
    "    last_conv_layer_name = \"conv2d_19\"\n",
    "    last_conv_layer = lenet_model.get_layer(last_conv_layer_name)\n",
    "    print(\"Last conv layer output shape:\", last_conv_layer.output.shape)\n",
    "    # Last conv layer output shape: (None, 14, 14, 128)\n",
    "\n",
    "    last_conv_layer_model = Model(\n",
    "        inputs=lenet_model.inputs, outputs=last_conv_layer.output\n",
    "    )\n",
    "    classifer_layer_names = [\n",
    "        \"flatten_3\",\n",
    "        \"dense_9\",\n",
    "        \"dropout_11\",\n",
    "        \"dense_10\",\n",
    "        \"dense_11\",\n",
    "    ]\n",
    "    classifier_input = Input(shape=last_conv_layer.output.shape[1:])\n",
    "    x = classifier_input\n",
    "    for layer_name in classifer_layer_names:\n",
    "        x = lenet_model.get_layer(layer_name)(x)\n",
    "    classifier_model = Model(inputs=classifier_input, outputs=x)\n",
    "    classifier_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIGURATION[\"DO_DRAW_GRAD_CAM\"]:\n",
    "    test_img = train_dir + r\"/happy/3159.jpg\"\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        test_img, target_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"])\n",
    "    )\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "        tape.watch(last_conv_layer_output)\n",
    "        preds = classifier_model(last_conv_layer_output)\n",
    "        top_pred_index = tf.argmax(preds[0])\n",
    "        top_class_channel = preds[:, top_pred_index]\n",
    "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
    "    display(grads.shape)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    display(pooled_grads.shape)\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    plt.matshow(heatmap.numpy())\n",
    "    plt.title(\"Grad-CAM Heatmap\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIGURATION[\"DO_DRAW_GRAD_CAM\"]:\n",
    "    import cv2\n",
    "\n",
    "    resized_heatmap = cv2.resize(\n",
    "        np.array(heatmap), (CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"])\n",
    "    )\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        test_img, target_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"])\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = img.astype(\"uint8\")\n",
    "    heatmap = np.uint8(255 * resized_heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * 0.4 + img\n",
    "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(\"uint8\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img.astype(\"uint8\"))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(heatmap)\n",
    "    plt.title(\"Grad-CAM Heatmap\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.title(\"Superimposed Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"grad_cam_happy_3159.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
